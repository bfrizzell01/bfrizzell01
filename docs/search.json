[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a recent Master’s graduate in Data Science with a background spanning statistics and machine learning research, math tutoring, and music instruction. That mix of analytical and creative experience shapes how I work — blending rigorous, data-driven thinking with curiosity and experimentation.\nI’m currently continuing to build my skills, filling in my gaps in my data science knowledge through self-education and personal projects like my music recommender system. I’ll be exploring topics like model deployment, API querying and hosting, visualization, dashboarding, front-end development, and web scraping. Stay tuned for more!"
  },
  {
    "objectID": "posts/spotify_genre_web_scrape/scrape_genres.html",
    "href": "posts/spotify_genre_web_scrape/scrape_genres.html",
    "title": "Web Scraping to Find Valid Spotify Genres",
    "section": "",
    "text": "Spotify uses genre tags to categorize music, powering features like song recommendations and playlist generation. Through the Spotify Web API, genres can also be used to search for tracks, artists, playlists, and more.\nHowever, there’s a catch: while Spotify recognizes thousands of genres, it doesn’t provide a straightforward way to access a complete list of them. This becomes a problem if you’re building applications that rely on valid genre inputs. In my case, I’m building a reccomender model, and I want users to have the ability to “pre-populate” the dataset with songs from their favorite genres, which requires having an up-to-date and accurate genre list to avoid empty queries.\nFortunately, several people have already compiled these genres and published them online. But given how long the list is, manually copying it would be tedious and error-prone. Instead, I used BeautifulSoup, a simple yet powerful Python library for web scraping, to extract the genres automatically and store them neatly for later use.\nimport requests\nfrom bs4 import BeautifulSoup\nimport pickle\nWe’ll start by sending a GET request to the blog page using the requests package. We’ll pass the\n# URL of the blog page\nurl = \"https://www.spudart.org/blog/six-thousand-spotify-genres/\"\n\n# Send a GET request\nresponse = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n\n# look at some of the text\nprint(response.text[:500])\n\n&lt;!DOCTYPE html&gt;&lt;html lang=\"en-US\"&gt;&lt;head&gt;&lt;meta charset=\"UTF-8\"&gt;&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;&lt;link rel=\"profile\" href=\"https://gmpg.org/xfn/11\"&gt;&lt;meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1' /&gt; &lt;style&gt;img:is([sizes=\"auto\" i], [sizes^=\"auto,\" i]) { contain-intrinsic-size: 3000px 1500px }&lt;/style&gt; &lt;!-- This site is optimized with the Yoast SEO plugin v25.3.1 - https://yoast.com/wordpress/plugins/seo/ --\nThe raw HTML is messy and hard to work with, which is why we use BeautifulSoup to clean it up for us.\n# Parse HTML\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n# Look at some of the text\nprint(soup.get_text()[:200])\n\n  6,119 genres on Spotify: try exploring them all - Spudart                                       Skip to content  FacebookBlueskyComicBlogNotepadArt projectsAboutContactEmail newsletter  Search    Se\nThe text comes as one line, so we’ll clean up by splitting on \\n newline characters and removing leftover blank lines\n# Find the main content section\ntext = soup.get_text(separator=\"\\n\").splitlines()\n\n# remove blank lines\ntext = [line.strip() for line in text if line.strip()]\n\n# print some of the cleaned text\ntext[:20]\n\n['6,119 genres on Spotify: try exploring them all - Spudart',\n 'Skip to content',\n 'Facebook',\n 'Bluesky',\n 'Comic',\n 'Blog',\n 'Notepad',\n 'Art projects',\n 'About',\n 'Contact',\n 'Email newsletter',\n 'Search',\n 'Search for:',\n 'Main Menu',\n 'Search',\n 'Search for:',\n 'Comic',\n 'Blog',\n 'Notepad',\n 'Art projects']\nLooking at the website in a browser, we know that the list of genres starts after the line: “All 6,119 Spotify genres” and ends before: “Enjoyed this blog post?”, so we’ll get all of the lines between these two lines.\n# filter for just genres\nstart = text.index('All 6,119 Spotify genres')+1\nend = text.index('Enjoyed this blog post?')\ngenres = text[start:end]\n\n# should have 6,119 genres according to the blog post\nassert len(genres) == 6119\n\n# show some of the genres\nprint(genres[:10])\nprint(genres[-10:])\n\n['21st Century Classical', '432hz', '48g', '5th Wave Emo', '8-bit', '8d', 'A3', 'Aarhus Indie', 'Aberdeen Indie', 'Abstract']\n['Zither', 'Zohioliin Duu', 'Zolo', 'Zomi Pop', 'Zouglou', 'Zouk', 'Zouk Riddim', 'Zurich Indie', 'Zxc', 'Zydeco']\nThis looks pretty good! But just as a quick sanity check, we’ll make sure all of these genres are indeed actually recognized by Spotify. But before we can do that, we need to get an access token from Spotify, since it uses OAuth 2.0 authorization. I won’t focus on the details of how this authorization works, but you can read more about it here.\ndef get_spotify_access_token(CLIENT_ID: str, CLIENT_SECRET: str):\n    '''\n    Retrieve an access token from Spotify using OAuth 2.0 authorization.\n    '''\n    \n    auth_str = f\"{CLIENT_ID}:{CLIENT_SECRET}\"\n    b64_auth_str = base64.b64encode(auth_str.encode()).decode()\n    \n    headers = {\n    \"Authorization\": f\"Basic {b64_auth_str}\",\n    \"Content-Type\": \"application/x-www-form-urlencoded\"\n    }\n    \n    data = {\"grant_type\": \"client_credentials\"}\n\n    response = requests.post(\n        \"https://accounts.spotify.com/api/token\", \n        headers=headers, data=data\n        )\n    \n    token = response.json()[\"access_token\"]\n    \n    return token\nfrom dotenv import load_dotenv\nimport os\nimport base64\nimport urllib.parse\n\nload_dotenv()\nTOKEN = get_spotify_access_token(os.getenv(\"CLIENT_ID\"),os.getenv(\"CLIENT_SECRET\"))\nBASE_URL = 'https://api.spotify.com/v1/search'\nheaders = {\"Authorization\": f\"Bearer {TOKEN}\"}\nNow we can cycle through the list of genres and filter out the unrecognized ones. For each genre in the list, we will:\ndef is_valid_genre(genre):\n    '''\n    Determine if a given genre is recognized by Spotify,\n    by whether or not it returns tracks for a query.\n    '''\n    params = {\n    \"q\": f\"genre:{genre}\",\n    \"type\": \"track\",\n    \"limit\": 1\n    }\n    response = requests.get(BASE_URL,params=params,headers=headers)\n    \n    if not response.json()['tracks']['items']:\n        return False\n    \n    return True\nvalid_genres = list(map(is_valid_genre,genres))\ngenres = [genre for genre,is_valid in zip(genres,valid_genres) if is_valid]\nlen(genres)\n\n5995\nIt’s a good thing we checked the list: there were about 120 genres in the original list that didn’t return any tracks, so we’ll throw those out.\nWe’re done! Now, we can pickle the genre list to use later.\n# store genres list\nwith open('../assets/valid_genres.pkl','wb') as f:\n    pickle.dump(genres,f)"
  },
  {
    "objectID": "posts/spotify_genre_web_scrape/scrape_genres.html#conclusion",
    "href": "posts/spotify_genre_web_scrape/scrape_genres.html#conclusion",
    "title": "Web Scraping to Find Valid Spotify Genres",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, I scraped a complete list of valid Spotify genres from an online source using requests and BeautifulSoup.\nBecause Spotify doesn’t publish its genre catalog directly through the Web API, having an up-to-date list is essential for building tools that rely on valid genre inputs - such as the personalized reccomender I’m working on.\nBy automating the extraction instead of copying manually, we ensure the data is accurate, reproducible, and easy to refresh if the source ever changes. This approach demonstrates how simple web scraping can be a powerful tool for supplementing gaps in public APIs."
  },
  {
    "objectID": "posts/optimal-design/index.html",
    "href": "posts/optimal-design/index.html",
    "title": "Supercharge Your Experiments with Optimal Design",
    "section": "",
    "text": "Designing experiments is as much of an art as it is a science, and in order to get the best insights from our data, statisticians and data scientists need to find creative ways to maximize the significance of their results. Optimal design theory gives us ways to design experiments that minimize the error in our experiment. I’ll talk a bit about where optimal design can be used, and show a simple example using simple linear regression."
  },
  {
    "objectID": "posts/optimal-design/index.html#informed-experiment-designs",
    "href": "posts/optimal-design/index.html#informed-experiment-designs",
    "title": "Supercharge Your Experiments with Optimal Design",
    "section": "Informed Experiment Designs",
    "text": "Informed Experiment Designs\nWe need to pick concentration levels that minimize parameter errors, and this is where optimal design comes in. We need to ‘squish’ the error of our parameters \\(\\beta_0\\) and \\(\\beta_1\\) into one single, summarizing value. To do this, we will use the D-Optimality Criterion, which, for our model, has the equation:\n\\[\n\\texttt{D-Optimality} = \\frac{\\sigma^4}{SS_{(\\text{fertilizer growth})}}\n\\]\nFor the math-savvy readers, this is the determinant of the covariance matrix of \\(\\beta_0\\) and \\(\\beta_1\\).\nWe want to minimize the D-Optimality criterion to get the lowest error in our parameters, so the bottom term, \\(SS_{(\\text{fertilizer growth})}\\), needs to be as large as possible. This is the sum of squared deviations in our 50 fertilizer growth levels:\n\\[\nSS_{(\\text{fertilizer growth})} = \\sum_{i=1}^{50} (\\text{fertilizer growth}_i- \\text{mean fertilizer growth})^2.\n\\]\nSo, to optimize our experiment, we should pick soil concentrations that are as far away as possible from their mean value. This means we should have 25 plants with no soil concentration, and 25 points at the maximum possible concentration. Let’s set up some code to test this result. I’ll pretend the true model has parameters \\((\\beta_0,\\beta_1) = (1,2)\\) and standard deviation \\(\\sigma = 0.25\\) for demonstration purposes.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# set up some simulation code\nB0 = 1\nB1 = 2\nvariance = 0.25\nN = 50\nnp.random.seed(123)\n\n# define some functions to calculate D-optimality\nSSx = lambda x: np.sum((x - np.mean(x))**2)           \nD_opt = lambda x: np.log(variance**2/SSx(x))\n\n# simulate some response data\ny_true = lambda x: B1 * x + B0\ny_error = lambda x: y_true(x) + np.random.normal(scale=np.sqrt(variance),size=N)\n\nNow, we’ll try out some experiment designs. One of them will be the evenly spaced measurements, the other will be our new optimal design. Don’t worry about the actual values, because we usually scale the design space to [-1,1] to make it easier for optimization.\n\n# make the evenly spaced design\nX_bad_design = np.linspace(-1,1,N)     \n\n# and the optimal dessign\nX_good_design = np.hstack(\n    (-1*np.ones(N//2),1*np.ones(N//2))\n)  \n    \n# plot reression line, and hypothetical data for each experiment\nplt.plot(\n    X_bad_design,\n    y_true(X_bad_design),\n    c='grey',zorder = 1)    \n\nplt.scatter(\n    X_bad_design,\n    y_error(X_bad_design),                         \n    color='red', zorder = 2,\n    label=f'Design 1: D-opt = {round(D_opt(X_bad_design),3)}'\n)  \n\nplt.scatter(\n    X_good_design,y_error(X_good_design),\n    color='green', zorder = 3,\n    label=f'Design 2: D-opt = {round(D_opt(X_good_design),3)}'\n)\nplt.xlabel('X');plt.ylabel('y')\nplt.legend()\n\n\n\n\n\n\n\n\nThe optimal design has a lower D-optimality, leading to overall lower error in our parameters! This was only a simple model, but we can extend optimal design to ANOVA, generalized linear models, etc.\n\nOptimal… But Good?\nThis design might reduce the error in our model, but is it actually a good design? We’re not really exploring the whole possibility of concentration levels in our experiment. Nevertheless, we now know where our model is most sensitive to error, and we can keep this in mind when designing our experiment. This was just a small purview into the ways optimal design theory can be used to sharpen your statistical insights!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Web Scraping to Find Valid Spotify Genres\n\n\n\ncode\n\ntutorial\n\n\n\n\n\n\n\n\n\nNov 2, 2025\n\n\nBenjamin Frizzell\n\n\n\n\n\n\n\n\n\n\n\n\nSupercharge Your Experiments with Optimal Design\n\n\n\nstatistics\n\ncode\n\ntutorial\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nBenjamin Frizzell\n\n\n\n\n\nNo matching items"
  }
]